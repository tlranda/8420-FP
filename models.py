import torch
import transformers
import tqdm
import re

import pdb

# Main inheritance class that defines the Train/Eval API all other models (subclassed) should use
class torch_wrapped(torch.nn.Module):
    def __init__(self, args):
        super().__init__()
        self.args = args
        self.tokenizer = transformers.AutoTokenizer.from_pretrained("google/bert2bert_L-24_wmt_de_en")
        # Have to use len(tokenizer) rather than tokenizer.vocab_size since the two former allows for things like [PAD] tokens to not break things
        self.embeddings = torch.nn.Embedding(len(self.tokenizer), args.nhid)
        self.create_model(args)
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=args.lr)
        self.loss_fn = torch.nn.CrossEntropyLoss()

    # Function to be extended/overwritten by subclasses, which should inject necessary modifications into the basic model architecture
    def create_model(self, args):
        self.model = torch.nn.Transformer(d_model=args.nhid,
                                          nhead=args.nhead,
                                          num_encoder_layers=args.nlayers,
                                          num_decoder_layers=args.nlayers,
                                          dim_feedforward=args.nhid,
                                          dropout=args.dropout,
                                          #activation,
                                          device=args.device,
                                          )

    def tokenize(self, de_or_en, **kwargs):
        # Minimal keyword args for success
        if 'return_tensors' not in kwargs:
            kwargs['return_tensors'] = 'pt'
        if 'add_special_tokens' not in kwargs:
            kwargs['add_special_tokens'] = False
        if 'padding' not in kwargs and type(de_or_en) is not str and hasattr(de_or_en, '__iter__'):
            kwargs['padding'] = 'longest'
        # May add special tokens like [CLS], [SEP], and [PAD]. Also note that BERT's tokenizer can break individual words into multiple pieces
        toks = self.tokenizer(de_or_en, **kwargs)
        return toks

    def embed(self, input_ids):
        # Process for creating embeddings
        return self.embeddings(input_ids)

    def forward(self, src_in, tgt_in, src_mask=None, tgt_mask=None):
        # Get embeddings and put them on device
        embed_src = self.embed(src_in).to(self.args.device)
        embed_tgt = self.embed(tgt_in).to(self.args.device)
        # Use model's forward method to process these, using the mask as needed
        # For simple single-pair case, there should not be masks (can supply None)
        return self.model(src=embed_src, src_mask=src_mask, tgt=embed_tgt, tgt_mask=tgt_mask), embed_tgt

    # Stably handles known exceptions as efficiently as possible
    def train_batch(self, all_examples=None, src_inputs=None, tgt_inputs=None):
        bad = set()
        # Tokenize if given all_examples
        if all_examples is not None:
            try:
                all_tokenized = self.tokenize(all_examples)
            except Exception:
                # NOT FOR PRODUCTION: Find UNK in training input and address it by adding to the .replace() chain
                # Rebuild and resubmit with clean input
                for idx, ex in enumerate(all_examples):
                    try:
                        tok = self.tokenize(ex)
                    except Exception as e:
                        #print("\n\n"+f"Tokenization error on {idx}: {ex}")
                        #if hasattr(e, 'message'):
                        #    print(e.message)
                        # Attempt to auto-solve
                        for char in ex:
                            try:
                                self.tokenize(char)
                            except Exception:
                                bad.update(char)
            #src_inputs = all_tokenized['input_ids'][:self.args.batch_size,:]
            #tgt_inputs = all_tokenized['input_ids'][self.args.batch_size:,:]
        # Forward may run OOM when things get pushed to device (or during optimization? unlikely but I'll catch it anyways)
        try:
            # FOR EPOCH SWEEP, SKIP THIS
            """
            outputs, target = self.forward(src_inputs, tgt_inputs)
            softmax = torch.nn.functional.log_softmax(outputs, dim=-1)
            # Optimization
            self.optimizer.zero_grad()
            loss = self.loss_fn(softmax, target)
            loss.backward()
            self.optimizer.step()
            # OK, but make sure to return .item() so we don't accumulate tensor memory
            return loss.item()
            """
            return bad
        except RuntimeError: # Will retry this part
            # Have to exit the try/except clause to de-allocate any tensors created in the block
            pass
        except Exception as e: # Weird bug, i think it's gone now
            print("\nunknown bug:")
            print(all_examples is None, src_inputs is None, tgt_inputs is None)
            print(e.message)
            pdb.set_trace()
        # Only reach this part if there's a problem
        # Recurse on half-sizes to rapidly handle issues. If only at one size anyways, we don't catch the exception
        # Split the batch but keep the tokenization
        n_inputs = src_inputs.shape[0]
        if n_inputs == 1:
            raise ValueError("Cannot recurse to sub-example level")
        return self.train_batch(src_inputs=src_inputs[:n_inputs//2], tgt_inputs=tgt_inputs[:n_inputs//2]) +\
               self.train_batch(src_inputs=src_inputs[n_inputs//2:], tgt_inputs=tgt_inputs[n_inputs//2:])

    # Performs all training for the given dataset
    def train(self, data, limit=None):
        self.model.train()
        aggr_loss = 0.
        n_examples = 0
        # set up batching
        dataloader = torch.utils.data.DataLoader(data, batch_size=self.args.batch_size)
        if limit is not None:
            end = min(data.num_rows, limit)
            # rephrase limit in batched terms
            limit = ((self.args.batch_size-1)+end)//self.args.batch_size
        else:
            limit = len(dataloader)
        progress_bar = tqdm.auto.tqdm(range(limit), desc='Epoch: ', leave=False)
        if self.args.device.startswith('cuda'):
            avail = f"{torch.cuda.get_device_properties(self.args.device).total_memory / (1024**3):.4f}"
        bad_chars = re.compile(r'[Ê»èˆ£ÊŠâ—ˆà¸åª›å¹¹æŒ‡å¸­å††ã·ãƒ¬é…æµ´é›»å‹•æ†©å‘‚ç”·å¤–æ­¢ã‚©è¡†è©±ãƒ‘è‹±æ¢…ç„¼å®¤è²¸è²©æ··å£²éŠ­ä¸¼ãéººæ¡è±†æ˜¥ç´ç…™ç¦ãƒé€Ÿã‚°ç¨ã‚¦ã‚·æˆ»ã‚§ãƒƒé‚„æ—…å®¿ãƒ›å½¹ç¥èº«ã‚å±…é¡Œã‚¯ãƒãƒ£ä»˜é¤¨åŠãƒ¦ä¼‘åˆºã‚­ãƒ¥ã‚³é®¨æ”¾é£²ãƒã‚¸æ©Ÿå†·ã‚„ç†±ç‡—æ¸©å†·ã‚„å¤®æœƒè®¾é‡è§†è¯‰èµµå°ç•¿æ˜Ÿç¨¿å„€è€³æƒ è¿›å‡ãƒ­æ­¤å¬å‹ä»ªå¼€å¸ˆå¯¹ËŒÊ²è¯¥æ•´ä¼ä¸¤åºåº·æ¯ç½²äº‰ç´°è®®è®¼å“¡é€ä¸´ä»…è´¨æ‹³å‘ä¸“å±ç•™å½»éŠ€ç–‘à¹‚å‘˜æš‚åçœç»é”€å¾—æ¯æ‹¬æŒ‰è»å¾‹åç»ˆè¿‘æ¡©å”®è‚¡ç°é›ªçº½ç§°ä½æ›è¦ªè°ˆåº¦ä½è°ƒè­‰å‡€ç®¡æäº’é—®é˜ªå‰§æ™¯æ¬¡å³æ—¶æ‹‰æ²¡é™…æä½™é”®æµä½œè½¬ç ´ç­¾å·¨ä¹¦æŠ¥æ´—ç¯æ˜¾åˆ¤ç»ŸÊ¿ç›‘åˆšçƒˆæŸ¥Ê€ä»ç”º ç…§å¿«éš›å€æ‹Ÿå‡ é¢œé…åˆ«è¶™å¼ºé‡‡è´­æ„å¤¹ã•å°†è¾ƒ è£…â™«è“Ÿèˆªä¹é­Î„ç½®å®£è„‘æ”¶ç›Šçº¦ä¿æ„å€ç ‚å€¼æ¯å¾¡ä¼°ä¼ è´·åŠŸå…‰å…³ç¬¬é½‹åº•æ‹’à¸¿é—¨æŒæ–™å·´æ‰“æ”¯è¯¦è§‰è¿·å¾ˆè°·è­¦è‹‘æ®åŒä¾¿è£‚é˜¿è”åº§é’¢å…¶è–Šé”ˆå®Œåˆ¥åŠ‰æ…è¯æ™®æ½®å–æ±‚å­˜è®¡ç§»ç«ä¸‡æœŸæ‰æ‰•ç§¯è¦è¯´åº—ä»æ“ä¸é©çœŸç‡å·²å¹¶ä»·åæ•°å†³å•†æ€¥æˆ–é€‰åŠå³çš‡åŒ…ä¹Ÿé“¶è¿°å†µé¢ äº¿å‡å¤´å¤ªä½—ç§Ÿè½¦éœ²ç»æš«çµ±å±â‚µå—æº¥åŠæ®µç‡•ç½—è¾¾æ’¤å‘Šé¢„å­£éœ€è¯„æå‚¨å·§åŒèµ„æç§‘Ê°æ¶ˆæµæ¬¾ç­‰ä¸šè¦ºæ¡ˆå˜æ°”ã¼é»ã‚ˆè¯†æ­Œå·¦æ›¸å¤æ§ä¹å§”â˜äº›æ ¼à¸¯æˆ¶å£³è®¯èƒŒè·å¸ƒåˆ™é™ç£¨ç¤ºé˜¶è«‹å°½æŠ•ã»é–¢é¸³Ê”é¸¯æˆ·íƒœé¦†è€í–¥áƒ®è³‡à¸ è¡¡É’É£Ú†à»œì¸µæ½¢éƒ¨ã‚»ì†ŒëŒ€çƒ§é•¿ä¼Šç´…ë²•áƒ«áƒ¦ç·çº¢çœ‹â‚¤ì æ¸¯åº“ç»çç¬ ì²œã‚ƒáƒ—åœ“ç¾½æ±‡æ¸‹àº½ä¼—è‘‰æ¨å»£É¨ç²¾å¾è‡´èŠ‹à¸æ•‘ç—´â‚ªãƒ“áƒ¬å¿…è«¸ã‚¢áƒ–å¥¶è‚‰ì¸å¤é»„éª¨ì‚¬éå…´ç è‡ºàºšå…·é€‚àº‚å¡Šå‡É¡ä½†çƒ­ç®æŠ€ë‹¹áƒªç¥­à»€àº¥å¹£æ‘©áƒ£çˆ†ãƒ àº²àº§å¿µè²»à¸©áƒ¥ì£¼êµ¬è‘£ë‹ˆÉ”åº«ë…„â–²à¸’èµ¤ë‹¤áŒ½ï¼’å…µë””éƒ‘áŒ¨ë€Œï¼„é¾á£ëŠ”é™ˆÉªèŒ¨ì¼ãƒ„áŠ£ì—ì›”æŸ”ÊŒâ–¸ë§ˆà¸¤é˜³ï¼ãƒ‹áŠ¦Å‹æ¢¨á‰€æ ªë¥´è¡“è¡—ìœ¼å€‰ã‚²á‹¨å¹¿á‰¶áˆì••é‚±ì„œÑ’Êƒï¼Šá‹Ò™ë¼è±áˆ…èƒ¡ë§‰ìŠµáˆ­É¹â¿ì—ˆÑ›ë¡œë ˆë‹Œæ¨ªè©¹áŠ­ì •ëª©Ê¾ä½•é–©é—½ï¼‘å–€à¸‰è»Šë°”é±¼á‰°ãƒ¢ãƒŸæœ”áˆ»ë¯¸à¸â‚©áˆ˜ä»€ë¸”å•¤áŒˆì§€Ğ‚æµœá‹­áŠ¥ë¬¸ì´áˆ¨×¥ï¿¥é¾™æ¡œå¿ áŠ ç«‹Ú­ç¦Û¶ğŒ¼æ¯›å‚³å…­æ°£àºˆæ°—é ğŒ½Éšåé‹Û·æ¡ƒå¦‚å®®è¡¨æ¿æˆ¸×—ä»£å¡Û´æ©‹ç«¶å—å£¹æ¯æŠ˜é˜ŸÚè…°å¯†ç£å…±áˆµæ¹¾é”¦æ¨“é“ğŒ´á‰µç»´Ø›å†™å¾è¨Šğ‰è¼ªå¾³å“¥ğƒéœŠğŒ¸å‘¼å²—Ûµè½®ç‹¬éµåœ’âœ¿æ¤É¬æµàº™ç–†å¨å·Ù¼æ³°å¯¶å°”å¯Œä¸¸åœ³èˆˆåŠ‡â”€ã€…å ´ğŒ°Ù¾å²¸å¡©Û³Ûè§’       è‚²å›­æ·è™Ÿå¤œè¦³å²©àº‡åŒ¡è…ºè‹åƒåŠ¡è»¢áˆ½å–«æ‘‚ãƒŒàª®é´»ç®•å¨„á€›è¦§à®®à¦•Ù²ë£¨å‰¯äº¨à¥¡æ­¥æ²ªì„å½­é¡¯æ›Œášæµ™ç‰¹ç­æ•£ç¹”é›™éŸ»Ë¤ç¨…è“®ç’ƒå¦»è©ç•¥à°•à²¶à¶±à¦™Ä¸ç°æ¢æ‹¼é£¼æ¡è¡€å¥½à¥‰ä»‹å¨£à´¨Şƒé»’è¼æ‰¿è–é½¡ã‚‡â…“æ´»àµ€ç¶™å¶¼æ…ã…–ç¶šæ•¦ãƒÚ¤ç›®é™¢èµ–é»ƒè­·å¥‡æ³¨à´¤å¨ƒå„‰å °â–’æ²™âˆ€å¹Œà¦¶Ù€å“®æ¤…ì•„äº‘å›ºà¤µæ£®é€ à¹‘ì˜é€¢ã‚œå©å›æ™ºà¤¨æ± å§‹éƒ¡æ²¹é‚£í‚¤ì¹˜ìŠ¤à®¤è¶…è±Ûæ¹˜å°¿âˆƒÉ¤æ…‹à¤¥é©è—é›¨ë”˜ä¼á€å©¦à¸®ã‚¨æ—­ì—˜áŠ«å§å¢©ì¡´à¥©ä½¿ì„¸æ€’é…‰à®·çœ¼ç¤±áˆå´ë…€æ›ì¶©í‡´è£•ãƒ§èª­ç¦ªæ±ºå¡æ£²ã…“å´©ìŠ¹à´¯è—¤å¿æœ­ãµì— æª€ï½ˆà¤ªæ ¹ï¼å¾½è‡£ë¹…ç‰™å¼„à¦†åš´á‹•à«¨à¦—åŒ–È»ê¹€à´ªç«ç²¤à²°Û•é›£é‹à®¯à®©à¦®ì œç´¹ã²æ¥µà¦¤áŠï¿¼æ›¹È¢ì¢…ä»®ã‚…ç†Šâ‰«æ¨¹á…ä¾í”¼à´…ë² Ûå‚áŸë‚˜ë ¤ä¹—ï½‚ç²µÊšà°‚å†°ç²‰æ¡¥å¼ æ…¶å›£à¥€ïºà¦‡è—ï»«æ•¬å“ˆè‘—ç›§ç¨²æ˜‡åâ‚ æŸ´è€…ãƒšç¿ åªè•­ä¿Šçº¿æ»¨è²æ³¢æŠ¼çˆ¶ìç´€é«”à®•â„“É«æ ¡á‰à´¿à°§áŠ å…ˆã›å¹¡ç´”á‹é—µèµ·à´¦áƒ¯é¹…â‚£æ¨™é£¯æ¡“ç­†å£®æ´‹ë§¨è©©ä½‘çˆ¾à´•åºœÊæ¬¢ä¸»ã‚±à¤·à°¶à´µã‡éŸ“á‘¦å¦¤ç±³å¤·áƒæ†¨å¹²å‰µë²ˆå¶ºä¹‹á‹²è„šãéµœæ‹“èª¿ï¿¡æ ½Û‡ç‚¯é …å’†åŠ›à¦‚ç›†Ëå¥šç†Ÿè¾²ÉæŸ±Ñ³å¼µà¸‘æºåªæ®»ä½à®Ÿï»™æŠ¹â”‚à²¾ã‚´æ…ˆê¸€å¯¸æ¾à¦²ç‰ˆæµ„è‡¨å¾©éœœå–‚é¡ë©´å–œè±«â”Œå‘¨á€±ïº©ï»ã…¡áˆªà¤‰ç‹—à¦°ç¯€â””ç™ºãƒªà³©å²¡å´áŠ•ç’°å‚·í†¨èå‹¿è°‹è³€æ—¨ì‚°è¬ë…¸ç´ ä¼¸ä»ä»™é›…ë³´à¤¬áˆ«é‡åŠ å€‹éƒ·åƒè€¶à½–Çí„´ä¹±à¤ˆì–‘â”¬ä¹‰æ²±Ê¤ç“Šå…§æ—à·€à¥§Ê¦é·¹ä¸˜ë³€æ¸ˆé™³à°œæ›¾èªªç—…å‘³è³ˆã‚½ì›¨é–€å¯³å‡„é¬¼å®‡åºšà¤¦áƒæ³‰å¿é€è©³àª•è„–à¯†áˆ¥à®™ë²Œà°ªàª¾à¶¸ï½•å¤«É¢åˆ‘å˜›áŠä»²çˆ›á’¥çŸ¢ç‘ à½‘å †áˆ™à®²åç‚ºå¯¨ìœ¨è˜‡á¢èˆ¬çŠ¶ã–è¨“æ¸¡æ¯“åŒ á“•éº©à®ªë‹¨å‚™ê¶Œæ˜­æ¡‚ç§€å®Ÿá€”à³®ä¹™ç›ç¸®á í™˜é›²é¨“á‰¢å½¢å™Œë™å†Šïºë°±æ²›áƒ™è´Šé’Ÿê³ ë±…à½£ë¦°ãƒ¨è¹Ÿè‰å®æ—§ä¸–è®²è¦æ˜Œæ”¹ä¹Œç‘©æ™‚è¾›è®°è…”ìš¸é®®àª¤ç­‘ä¹…éˆ´é„­ç“¶ç§é·æ¯çƒæ²¢å–‡áŒ‹à´®é²à¦Ÿå¬ï¿ ã‚‘å†Œæ©å› à¯‚æ´è­¯æ¿±å±¯Éœç¿»æ¡¶ì „à¯‹à´áˆœåœã…•ä¼¯éšŠæšã‚æšï½…ì•Œß–à¤¶æ•™å¥ˆé ­É¾è´§á‘²ç‚¹ì™•æ¾„ì…€ê³¡æ±‰ìš”è¿áˆ´è¼‰à°—ìˆ˜ï¼³à¦·ìœ é †æ–¯ë„ï¼Ÿå¹¸Ë¡à¤œì–´è¨±æ³ï¼›Û†å¯§è”£É²Ø¸àµ‹ï¼»æ¸¸â”˜à¥¨àª°ç•¢æ— é˜²å ‚à´²áºè¾¼à¤£é»¨á å›§å‹ã‚£åœã‚¶é³³à¸†å ±é³©æœë¹„å®—æ³½èŠà½˜åƒì‹ í¼èª¬é¼»à«¦åˆ€ç¶“ç››ï¼—é€¸á“„çˆªæ­è¨­å¸à²¯ì¡°à¼‹ç„¡å­¸è©¦ãƒè—•æ»í•œá‹‹æ½­éš†æ°å‹¢â„®ãƒ”é‚‘é¸­æ¾³ë¬´ì„ é››æœãƒ´ã‚¡Ê¼å¸ŒíŠ¸ç™»à§‹ï½ŒÛ¸àªœé‰„Êá’ƒãƒå³½ê³„é ˜ä¹¡å¼à°°í´é¯¨å¾Œé£à²µå·±à¤†ç™¾æ‹åŸ”å¤¢åå“²à°¦à®µéŒ²èœé ˆà¦¾å¸¥á‰£å²·æ³Šå¾…ç”»èŠ¦à°¡è·†æ°¸à¦ç»é»æ±•å‘‰â˜†å£ç›£ì•¼å¼¦æ¡ç—›àª¿æ•…à®šå‘ç‚åƒâ”å¸«â‚çš®Û‚à¥‹æµ®ã‚µã…”å¿—æ’«å¾®à³¬ã¶í‹°Ñ²à³¯è€í‚¨ì‹œæœ—ãƒ¯å£½é¸¡ç¢ºá€Ÿãƒ™à³¨à¤›ç¥–è·àª…æº¢æ¼«à¤šà¯ã‚›à¦¹å‡æš¨å‹§à³§à½ åº†æ‚Ÿà¦¨å˜‰áˆ°è¨³à²â™‚å¢—æ‰ç¸£à½é–“à³ˆà«§å¼“å®‹à¸Ÿï½ç« ä¸¹éƒê²€å‹éœ–èˆŠêµ­ç‰›æ±ë¥˜ëˆ„å‰£ç´šë¹Œé¹¤à¥¦é›†å±€å ¤å™›ä¹¾æ»‡ë“œè”šâ™ Ş‰æ¥­ç¦ç‹æ¿Ÿé™µà®°à´¸é„‚Ú¯ç´‚ï½„à±í˜¸é±²ì…”æŒ‚ç¾¤å²­ç¶ºä¹˜à³«Ëœåˆ—è«‡é”ç”¸è±Šå…©é©—ë”©æ½Ÿæ¤œä»Šå´–â‰¡è³¢ç¢§å²é£à°¨ç§¦äºœà¦ªå¥˜êµ°ç”³à·„æº«é¦–å¡ï¼½ã‚€è© ê²œèœ€å‡½àª¸å‰‡à¦¯ç„æ¾¤æƒà³–å†œåŸºà¤¤æ›åŸŸâ‰ªà¤ŸáŠ¤Ù¹ç¾©å”ç¥¿æ…£æ»¬à®¾ì‹±à§°è´¤æà¥«áˆŠë„í„°ãƒà³¦å«áˆà¯ˆç®±å®°à¶»æ²¿å®æ€ªå®Ú¾é¡—ã­éº—å±¿è£½ïº¤æºªà¥¯Æ†à´· ]æ­¸è‰â…›æ’­å»ƒé«„å¤„à©œé™¤åšå¢æº€è‡‚å´”é‚Šç´§æƒ§ç™ŒÆ›è§ï½¥ä¼˜ä¸ƒæƒ‘ëŒâ„ƒğ§µ®ç§‹è¶‹ã‚¥æ†¶æ´å«æº¶ç©·é‡‰å²³ã€•ç»æˆªæ•ˆé‹–âŸé°„è®¤æ€â”æ°·è¾±å°¾èƒä»»æ·¡æ’è´Ÿè¸Œåƒ•é¥²æ³„é‹ï½”éšœéˆè®Šå·»à½šÊ‹è²Œè®ƒæ„Ÿæ‡‰æ™è€€éšæŠ¤æ´›é°Šæˆ˜æ ·á€æ¡è²¡è·Ÿò´©§â”´ç„é²ì‘å¦è›‹å¨æ²ˆì²´å˜Ÿå±•é€ƒç”±1é¥®èµ¶ç¼©ÊºÉµå°‚æ»æ‚æš´é€”Û©â€–å¾Æ˜ç«¹ã€”æ¹„æ¶©ä½°ä¿„åˆ¨è´±è†šå®ƒä¼â˜‘ç™ªä¼Ÿä¸¥è¯‘]ç‹­å¨¥å’¬å¥„æ­©æ€¡ç­”è‡ˆæºçŸ¿è•´àº¡è¿½å±åŒ¹é”™èºèˆ«â„”æ®·è¶³èŠ‚è·‹åºŸï¾Ÿè¯ºà¥â…¢å‰‘å›°ğ² ¤å¯¦æ˜ â²ë¹›èˆ…å¦¨â–ˆèƒæœ›â†“Æ“æ¬§å¿˜å–„ä¸¾è¡£ç†¨å¯„é—»ç€ç½ªèºæ‘†âˆ¶é—ç²é é§„é³å´æº‰æ™¨ä»¬Ú¬é‹œá‹µæ ‘â†§ì°½æ¸´ï½¿å¼‚ç•ªå±ˆÉ´å‡†âˆ´é¨œé‹˜à¤­æƒ¯ò²¤¥æ¿€â†”å¥¸é’§à¨—ç¡®è„æ°®æŒ è´ãƒœå¿»å³»é é¢ç¾â²‰é™¶ç¾²é—´èˆå§†è¯·ç²é¿ç”œèµ›å¡—ç„¶ä¸¨ç–äºå¹Æ–ãè¹å‡‰ä¹ è‘¡é‹Ÿà¸‹ä»–ì›Œé‹‘å“æŠœ ç¯‡å¸¦æ‹›è¨è£å‘€å²è²¢é»®É•è¨â…ç»ªà¤§ç›–åº„âŒƒå‡¶ä¾›æ­»æ”¿ğ „è¿œß§äºï¼°è¨ƒçµå¹¾ìœ„ç„¦àº–æµ‹ç¡¬ç’å­´æ¸å—£å½°Ä¿å•é»µç²ë‹˜ç å„é–¥é’ˆç»†à½“ğ² „å¤•ê²½ì¶”çŒªâ–³í›ˆå¤à®¿æŠµè·³åœ–à¤ä¼é‹“å¤Ÿèµğ¢¥²å«â² å€å“æºé’¦åå”¸ç–—âˆ™æ·¹é™€ò®¬©Æ¬è¼ƒ å±„æº¯åº­ç´™é’±é£›æ‚ªç‰ì…œé‡£å¡”å¾¹è®¸ç§‰çŒ›ç«¯å‰©è°¢Ğ‹â‡—à°†æ¶é“¾ãƒ¤å®œë°•è–¯è¾­çµ‚ç­–ç¢äº§æœé‹ã€œâ…¡æˆ¿è²®èƒœå†å†¤ã„†é¹¿â˜çµæ–½                                                                                                                                                                                                                                           ç”²èŒ‚èŒœË˜ç¦½èª“é–£è¨âˆ«å…»ã‚†Ë‘èƒ–æ¶‚à¦šà¸è™‘é™½çœ©è‚¥ç˜¤å½¼è€ƒÈ¤â‚´æŒ¡è¯¾âšç¼ºå³ç»­å¥‰é‡ä¸°é™©á‹°è¯˜à¸ï·²áˆ•è‰®ç¨¼æ½œæœ±â•æ®¿Ş‡é…¸å•ä»¶æœªæ¸å¤‰çˆæ®Šéš¾æ‹¿ç°¡éƒé»¡ç¿’ã„§æ£€é¨™è¿«ç¢é»·é™é¡·ë°˜è‡³ä¼´ç¿¼à¤—çº§áâ‘¢å¡¾å¾ªçºªåŒ»æœ«é€†ï½«åºíæ‰¾é“ƒæ´’æ±´ï¼©æ²³è¯•ç¢è¾‘é°ˆæ‚æºœåˆ›ç›—â–‘ã¬è´¹à©€éŠíšŒè€•æ¢à½¦âŒ˜é¡¾å‘†æˆ’ç²®å¤‡èˆâ…”è§‚å¿†æ‚£è…•ç¥¨åˆ¶å¿½è‰¯ë³„è€¸è’¸ä¾‹çº¹é›¢è„±æ¥ ã„‡é—œè² æ³¥çƒ¦æœ¯ã¡ï¾œè¿”ï½»ãœæŸ“ç»„æŒ‘â—‡æ†æ©¿â´æ´å‰²é»šèºŠè²´à¨šã„…å¦–ç›å±Šçˆ»è–‡å®«æ¸›å¾’è®©è´¡â˜»é ‘æ´ªæ–—çº·æ‚¨é°ƒæ†¾å¤â‡”æœç¥¸é€£å£Šèæ¥¼æ¥è´«å½©å®à¤²ä¸½è©°ç¶²âˆˆç…©æŸë ¹è¬‘àº”ç«œé˜»àª¯æŸå†›ç´‹è‡§å§¦åŠ³æ‰ˆææ°§æªæ ‡â‘¡å¯»â—¦é£å‘‘ä¹ˆï½²ä¸ˆæ‹…æ£’å¥¥à¤¡ã„¤æ’’ï½“é„‰æ¢Æ¦é‰‹ì„±è«–é™°åƒã„ˆä¾ç¨»É¯èŸ²à¤¯è™«é•œå°˜å›è”“è¿æŠ—æŸï¸°è–¬ç³–è‡˜ä¹éšÚ èˆç •åµ¥âˆ¼ç‡ƒå«Œæ²Œâ—™éªò²µ®æ‚©ä½â²£ç´¢è¯é”‹é¹ˆäŸ©æ¢°à¨¹Ò³æ±¤Ë‡ç…®è’‹å…°áˆ“â˜¼à¤¢è½»ç³¸é™æ‚ é–‹çŒ®è‹¥æå¦å¥ã…£é‹å¦Êç‰Œâ–¡è‡“é½â€¼å¢ŸåŠ¿é“­åŠªì§æ”»è¿˜å¤›åŠ©æ‚¬ã‚¾æ¶Œæ¨±å¸¯å¿œâ€¶çÒ²æ’®æ€»è§¦èˆ©å¼”æ—‹èˆ–è¦–â‹…å…½çº¸ã‚®ç¨€à¨¡àº„ã³ï½¤æ‹©ãƒç³å·®æ…¢å¡æ—±é°€ï¼™ãƒ’æ´¾ì˜¤è“‹ìæ‘‡ç£å€™Ä² è–”âˆ‘â˜¯å“å‡¸ç¹é’“ç¨‹ç¼“æ¡†â‘ ò®®¥â°†ç£·é°è¥è³è¾è´¸â²Ÿç¿ã‚¼å®˜ç¢³è´³â˜ºï¼¶æŒ¯å€’èé¡»Ş†  è·¨                                                                                                                                                                                                                                           ç¤¼å‡Ææ­³ç³»åŠç¡ç¨±ãƒ˜ç¯èº‡ä¹°â€½æƒ³å‹éçŸ­é˜œé²æ¸²ç¦¹ç¨³çµŒå‘½éªŒæˆ´é­”ç±»ä¼¼é¡µè®£è²°å¯¤â²©ì›…æ´²é‹’çŒç•Œåœ†')






        skip = 8070
        def str_fixer(s):
            return bad_chars.sub('', s)
        # DEBUG SKIPPER
        all_bad = set()
        for it, example in enumerate(dataloader):
            if it < skip:
                if self.args.device.startswith('cuda'):
                    progress_bar.set_postfix(alloc=f"{torch.cuda.memory_allocated(self.args.device)/(1024**3):.4f}", reserved=f"{torch.cuda.memory_allocated(self.args.device)/(1024**3):.4f}", max=avail)
                else:
                    progress_bar.set_postfix(mem="running on cpu")
                progress_bar.update(1)
                continue
            if limit is not None and it >= limit:
                break
            all_examples = [str_fixer(_) for _ in example['translation']['de']]
            batched = len(all_examples)
            all_examples.extend([str_fixer(_) for _ in example['translation']['en']])
            # Will handle OOM and known tokenization issues
            #aggr_loss += self.train_batch(all_examples=all_examples)
            all_bad.update(self.train_batch(all_examples=all_examples))
            if len(all_bad) > 1000:
                print(''.join([_ for _ in all_bad]))
                raise ValueError("Time for restart")
            n_examples += batched
            if self.args.device.startswith('cuda'):
                progress_bar.set_postfix(alloc=f"{torch.cuda.memory_allocated(self.args.device)/(1024**3):.4f}", reserved=f"{torch.cuda.memory_allocated(self.args.device)/(1024**3):.4f}", max=avail)
            else:
                progress_bar.set_postfix(mem="running on cpu")
            progress_bar.update(1)
        print(''.join([_ for _ in all_bad]))
        return aggr_loss / n_examples

    # Performs single-example inference for evaluation pipeline
    def evaluate(self, de):
        with torch.no_grad():
            tokenize = self.tokenize(de)
            outputs, _ = self.forward(tokenize['input_ids'], tokenize['input_ids'])
            outputs = outputs.cpu()
            li_strs = []
            for bid, batch in enumerate(outputs):
                # Get per token argmax by nearest embedding (best guess at intended word)
                words = [torch.argmax(torch.norm(self.embeddings.weight.data - word, dim=1)) for word in batch]
                # Convert these tokens back using the tokenizer
                li_strs.append(self.tokenizer.decode(words))
        return li_strs

# Attributes to be accessed from this file as a module
choices = ['default']
models = [torch_wrapped]
lookup = dict((k,v) for k,v in zip(choices, models))

